{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42aa3f54",
   "metadata": {},
   "source": [
    "# Testing the pure-LDP library with actual benchmark datasets\n",
    "To apply the LDP mechanisms in our research we will use a python library that has implementations of all LDP mechanisms that we will use. The library is from Samuel-Moddock and is called 'pure-ldp'. The link to the github page: https://github.com/Samuel-Maddock/pure-LDP.\n",
    "\n",
    "To be certain this library will work we need to conduct some tests on datasets that we will use for our benchmark tests later in the project.\n",
    "\n",
    "So first let's load some LDP mechanisms from the pure-ldp library. For this we will use local hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be3d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pure_ldp.frequency_oracles.local_hashing import LHClient, LHServer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061b56b",
   "metadata": {},
   "source": [
    "The first test is the example provided by Samuel Maddock's github page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36bd4996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3911.826679014247\n"
     ]
    }
   ],
   "source": [
    "# Using Optimal Local Hashing (OLH)\n",
    "\n",
    "epsilon = 3 # Privacy budget of 3\n",
    "d = 4 # For simplicity, we use a dataset with 4 possible data items\n",
    "\n",
    "client_olh = LHClient(epsilon=epsilon, d=d, use_olh=True)\n",
    "server_olh = LHServer(epsilon=epsilon, d=d, use_olh=True)\n",
    "\n",
    "# Test dataset, every user has a number between 1-4, 10,000 users total\n",
    "data = np.concatenate(([1]*4000, [2]*3000, [3]*2000, [4]*1000))\n",
    "\n",
    "for item in data:\n",
    "    # Simulate client-side privatisation\n",
    "    priv_data = client_olh.privatise(item)\n",
    "\n",
    "    # Simulate server-side aggregation\n",
    "    server_olh.aggregate(priv_data)\n",
    "\n",
    "# Simulate server-side estimation\n",
    "print(server_olh.estimate(1)) # Should be approximately 4000 +- 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763e1c8",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "For the datasets we will use the same datasets that were used in the paper 'Comparing Classifiers’ Performance under Differential Privacy' by Lopuhaä-Zwakenberg et al. \n",
    "\n",
    "The first to load is the 'Adult' dataset. This dataset has some different attributes like age, race, sex, education etc. The target attribute is a value that indicates if the person has an income of >50K or <=50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6cf720f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of the first 5 rows:\n",
      "\n",
      "    age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
      "0          2174             0              40   United-States   <=50K  \n",
      "1             0             0              13   United-States   <=50K  \n",
      "2             0             0              40   United-States   <=50K  \n",
      "3             0             0              40   United-States   <=50K  \n",
      "4             0             0              40            Cuba   <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"./Dataset/adult.data\")\n",
    "\n",
    "print(\"An example of the first 5 rows:\\n\\n\", raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433e97c",
   "metadata": {},
   "source": [
    "## Discrete data test\n",
    "The dataset contains discrete and continuous data. \n",
    "Let's first look at the discrete data. Some questions that arise are: Do we need to encode the data into numerical values? \n",
    "From the github example the pure-ldp functions take numerical data so we need to construct an encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ee9e9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_discrete_data(data):\n",
    "    features = data.columns\n",
    "    \n",
    "    for feature_name in features:\n",
    "        data.loc[:,feature_name] = data[feature_name].astype(\"category\").cat.codes.copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1bd3a",
   "metadata": {},
   "source": [
    "The columns with discrete data are: workclass, education, marital-status, occupation, relationship, race, sex, native-country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f5a5f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of unique values:  {0: 1836, 1: 960, 2: 2093, 3: 7, 4: 22696, 5: 1116, 6: 2541, 7: 1298, 8: 14}\n",
      "The estimate by the olh frequency oracle for the number 0:  {0: 1808.3203571105807, 1: 898.4904720551697, 2: 2083.14311415249, 3: -19.667375334844337, 4: 22865.573120155033, 5: 1110.853511587554, 6: 2603.6407600651955, 7: 1337.7904852054937}\n"
     ]
    }
   ],
   "source": [
    "discrete_feature_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "\n",
    "# select only the data from the discrete columns\n",
    "discrete_data = raw_data[discrete_feature_columns]\n",
    "# convert the data from category names to numbers\n",
    "discrete_data = convert_discrete_data(discrete_data)\n",
    "discrete_data = discrete_data.to_numpy()\n",
    "\n",
    "# Reload a new instance of the client and server functions of the local_hashing frequency oracles.\n",
    "epsilon=4\n",
    "d=9\n",
    "client_olh = LHClient(epsilon=epsilon, d=d, use_olh=True)\n",
    "server_olh = LHServer(epsilon=epsilon, d=d, use_olh=True)\n",
    "\n",
    "for item in discrete_data:\n",
    "    # Simulate client-side privatisation\n",
    "    priv_data = client_olh.privatise(item[0]+1)\n",
    "\n",
    "    # Simulate server-side aggregation\n",
    "    server_olh.aggregate(priv_data)\n",
    "\n",
    "# sum the unique values\n",
    "unique, counts = np.unique(discrete_data[:,0], return_counts=True)\n",
    "\n",
    "print(\"The sum of unique values: \", dict(zip(unique, counts)))\n",
    "print(\"The estimate by the olh frequency oracle for the number 0: \",{i : server_olh.estimate(i+1) for i in range(8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ef243",
   "metadata": {},
   "source": [
    "### Note\n",
    "The frequency oracle functions use numerical inputs that start from 1 and not from 0. This should be kept in mind when specifying the domain size (`d`) and the estimated value (`.estimate(...)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa42661",
   "metadata": {},
   "source": [
    "## Continuous data test\n",
    "Since the LDP algorithm needs categorical data there is a need to categorize continuous data. We can do this by binning the data. This means we will split the range of the data into different bins that each represent a range of values.  \n",
    "\n",
    "Pandas has a function `pd.qcut(...)` that does this automatically. It distributes data in bins of equal sizes. The number of bins is specified by the `q` value. The `labels` value is set to `False` to indicate that we want numerical values for the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "64e48919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week\n",
      "0    5       1              4             0             0               2\n",
      "1    7       1              4             0             0               0\n",
      "2    5       6              1             0             0               2\n",
      "3    8       7              0             0             0               2\n",
      "4    2       9              4             0             0               2\n"
     ]
    }
   ],
   "source": [
    "continuous_feature_columns = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "continuous_data = raw_data[continuous_feature_columns].copy()\n",
    "for feature in continuous_feature_columns:\n",
    "    continuous_data.loc[:,feature] = pd.qcut(continuous_data[feature], q=10, labels=False, duplicates=\"drop\")\n",
    "\n",
    "print(continuous_data.head())\n",
    "\n",
    "continuous_data = continuous_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da2b14",
   "metadata": {},
   "source": [
    "### Applying LDP to binned continuous data \n",
    "Now that the data is separated in bin we can try and apply the pure-LDP frequency oracle to see if it will correctly estimate the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "02d246f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of unique values:  {0: 3895, 1: 3301, 2: 3376, 3: 2591, 4: 3518, 5: 3245, 6: 3008, 7: 3167, 8: 3461, 9: 2999}\n",
      "The estimate by the olh frequency oracle for the number 0:  {0: 3921.5407995161686, 1: 3305.2715867555244, 2: 3407.289125354415, 3: 2557.836967224878, 4: 3457.2568993620343, 5: 3459.338889945686, 6: 3059.5966978847273, 7: 3194.9260858220305, 8: 3378.1412571833034, 9: 2932.5952722820266}\n"
     ]
    }
   ],
   "source": [
    "# Reload a new instance of the client and server functions of the local_hashing frequency oracles.\n",
    "epsilon=4\n",
    "d=10\n",
    "client_olh = LHClient(epsilon=epsilon, d=d, use_olh=True)\n",
    "server_olh = LHServer(epsilon=epsilon, d=d, use_olh=True)\n",
    "\n",
    "for item in continuous_data:\n",
    "    # Simulate client-side privatisation\n",
    "    priv_data = client_olh.privatise(item[0]+1)\n",
    "\n",
    "    # Simulate server-side aggregation\n",
    "    server_olh.aggregate(priv_data)\n",
    "\n",
    "# sum the unique values\n",
    "unique, counts = np.unique(continuous_data[:,0], return_counts=True)\n",
    "\n",
    "print(\"The sum of unique values: \", dict(zip(unique, counts)))\n",
    "print(\"The estimate by the olh frequency oracle for the number 0: \",{i : server_olh.estimate(i+1) for i in range(d)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
